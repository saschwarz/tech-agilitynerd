<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>tech.agilitynerd - devops</title><link href="https://tech.agilitynerd.com/" rel="alternate"></link><link href="http://127.0.0.1:8000/feeds/devops.atom.xml" rel="self"></link><id>https://tech.agilitynerd.com/</id><updated>2016-09-23T18:00:00-05:00</updated><entry><title>HTTP/2: A Quick and Easy Website Speed Up</title><link href="https://tech.agilitynerd.com/http2-a-quick-and-easy-website-speed-up.html" rel="alternate"></link><published>2016-09-23T18:00:00-05:00</published><updated>2016-09-23T18:00:00-05:00</updated><author><name>Steve Schwarz</name></author><id>tag:tech.agilitynerd.com,2016-09-23:/http2-a-quick-and-easy-website-speed-up.html</id><summary type="html">&lt;p&gt;I always want my websites to be secure and fast. &lt;a class="reference external" href="https://http2.github.io/"&gt;HTTP/2&lt;/a&gt; is the latest enhancement to the HTTP protocol that can provide significant performance improvements:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;The primary goals for HTTP/2 are to reduce latency by enabling full request and response multiplexing, minimize protocol overhead via efficient compression of …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;I always want my websites to be secure and fast. &lt;a class="reference external" href="https://http2.github.io/"&gt;HTTP/2&lt;/a&gt; is the latest enhancement to the HTTP protocol that can provide significant performance improvements:&lt;/p&gt;
&lt;blockquote class="epigraph"&gt;
&lt;p&gt;The primary goals for HTTP/2 are to reduce latency by enabling full request and response multiplexing, minimize protocol overhead via efficient compression of HTTP header fields, and add support for request prioritization and server push.&lt;/p&gt;
&lt;p class="attribution"&gt;&amp;mdash;&lt;a class="reference external" href="https://hpbn.co/http2/"&gt;High Performance Browser Networking - O'Reilly&lt;/a&gt;:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It turns out all browsers that support HTTP/2 &lt;a class="reference external" href="http://caniuse.com/#feat=http2"&gt;also require TLS (HTTPS)&lt;/a&gt;. So my first step was &lt;a class="reference external" href="https://tech.agilitynerd.com/nginx-https-lets-encrypt-and-django.html"&gt;adding HTTPS support to agilitycourses.com&lt;/a&gt; which also provides a backbone on which I'll implement secure user profiles. Then I wanted to enable HTTP/2 to see if it improved the speed of pages served to end users.&lt;/p&gt;
&lt;div class="section" id="but-first-an-os-upgrade"&gt;
&lt;h2&gt;But First an OS Upgrade&lt;/h2&gt;
&lt;p&gt;After a little research I found that recent releases of &lt;a class="reference external" href="https://nginx.org/en/"&gt;NGINX&lt;/a&gt; support HTTP/2 and those versions are packaged with Ubuntu 16.04 LTS. That made upgrading to this latest long term support OS version a better approach than just upgrading NGINX on my older OS. I basically went through the &lt;a class="reference external" href="https://www.digitalocean.com/community/tutorials/how-to-upgrade-to-ubuntu-16-04-lts"&gt;standard upgrade process&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While I had to work through a number of small issues upgrading my older websites; the biggest change was converting my upstart scripts to systemd scripts. But the beauty of hosting with virtual servers is it was trivial to create an image of my existing server, spin up a temporary server using that image, and then practice the ugprade/migration on the temporary server. I was able to move some configuration changes back to my live server and test out the changes to my sites' &lt;a class="reference external" href="http://www.fabfile.org/"&gt;Fabric&lt;/a&gt; deployment scripts.&lt;/p&gt;
&lt;p&gt;Once everything was working I configured my websites that contain user modifiable data into maintenance mode on my temporary server. Then I pointed the DNS for all my domains to the temporary server. After DNS propagated I shutdown Postgres and the webserver on my original server and created a backup image. Then I went through the upgrade process. Once I validated the migration was successful I switched the DNS back to my original server and shutdown the temporary server. After a couple days I deleted the temporary server and the pre-migration backup image.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="enabling-http-2"&gt;
&lt;h2&gt;Enabling HTTP/2&lt;/h2&gt;
&lt;p&gt;Once I was on NGINX version 1.10 the change to the website to enable HTTP/2 was as simple as changing the virtual server from:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
server {
    listen 443 ssl;
    listen [::]:443 ssl;
...
}
&lt;/pre&gt;
&lt;p&gt;to:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
server {
    listen 443 ssl http2;
    listen [::]:443 ssl http2;
...
}
&lt;/pre&gt;
&lt;p&gt;Then I just reloaded the Nginx configuration: &lt;tt class="docutils literal"&gt;sudo systemctl reload nginx&lt;/tt&gt;. That was it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="testing"&gt;
&lt;h2&gt;Testing&lt;/h2&gt;
&lt;p&gt;Here's the network view of the timing of requests for a page using HTTP/1.1 over HTTPS in Chrome:&lt;/p&gt;
&lt;img alt="Diagram of HTTP requests for a page showing requests being delayed by previous requests for the same domain." class="thumbnail" src="/images/agilitycourses-http1.png" /&gt;
&lt;p&gt;Here's the same page using HTTP/2 over HTTPS:&lt;/p&gt;
&lt;img alt="Diagram of HTTP requests for a page showing requests being multiplexed with previous requests for the same domain." class="thumbnail" src="/images/agilitycourses-http2.png" /&gt;
&lt;p&gt;The obvious difference is in the &lt;em&gt;Timeline - Start Time&lt;/em&gt; column. In the first diagram you can see the &amp;quot;waterfall&amp;quot; queueing up of requests for images as all the open sockets to the domain were in use. In the second diagram you can see them all interleaved as they are multiplexed across the same connection.&lt;/p&gt;
&lt;p&gt;Also the bottom line is the page is fully loaded in 1.73 sec using HTTP/1.1 and in 1.16 sec using HTTP/2 for a &lt;strong&gt;33% end user page load improvement!&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="next-steps"&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;So that was a nice free speed up and there are plenty of areas I can still investigate to further improve the performance of this web site:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Bring 3rd party resources back to my domain to save DNS lookups and see how that affects overall performance. This may hurt performance for HTTP/1.1 clients who are limited to the number of connections per domain.&lt;/li&gt;
&lt;li&gt;Change multiple SVG files (which are all steps in a progression) into a single file and use CSS to hide/show appropriate steps using a single image. This will greatly reduce the number of individual files downloaded and save some duplicated data within those files.&lt;/li&gt;
&lt;li&gt;Move Google Analytics locally. GA has a number of problems (short cache times and multiple files downloaded) so at the cost of periodically updating the files I could host them all locally and save some time.&lt;/li&gt;
&lt;li&gt;Disqus is even worse than GA when it comes to many file downloads, redirects, short cache times etc. I might change to have Disqus data loaded on user demand.&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;Experiment with other HTTP/2 features:&lt;/dt&gt;
&lt;dd&gt;&lt;ul class="first last"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.igvita.com/2015/08/17/eliminating-roundtrips-with-preconnect/"&gt;preconnect&lt;/a&gt; might help reduce the cost of DNS look ups that result from redirects like those used by Google Analytics, Google Fonts, and Disqus resources.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.smashingmagazine.com/2016/02/preload-what-is-it-good-for/"&gt;preload&lt;/a&gt; to load resources before they are accessed by JS during &lt;tt class="docutils literal"&gt;onload&lt;/tt&gt; or via user actions.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Link_prefetching_FAQ"&gt;prefetch&lt;/a&gt; to load JS/images for future pages. There are some common workflows on this site that could benefit from prefetching the JS for subsequent pages.&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="nginx"></category><category term="ubuntu"></category><category term="https"></category><category term="ssl"></category><category term="http2"></category><category term="performance"></category></entry><entry><title>NGINX, HTTPS, Let's Encrypt, and Django</title><link href="https://tech.agilitynerd.com/nginx-https-lets-encrypt-and-django.html" rel="alternate"></link><published>2016-09-04T12:02:00-05:00</published><updated>2016-09-04T12:02:00-05:00</updated><author><name>Steve Schwarz</name></author><id>tag:tech.agilitynerd.com,2016-09-04:/nginx-https-lets-encrypt-and-django.html</id><summary type="html">&lt;p&gt;My &lt;a class="reference external" href="https://agilitycourses.com"&gt;agilitycourses.com&lt;/a&gt; website is served by &lt;a class="reference external" href="https://nginx.org/en/"&gt;nginx&lt;/a&gt; proxying to &lt;a class="reference external" href="http://gunicorn.org/"&gt;gunicorn&lt;/a&gt; running my &lt;a class="reference external" href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; application. I'll be adding user accounts soon so I wanted to convert the site to be more secure by using HTTPS encryption. Also &lt;a class="reference external" href="https://webmasters.googleblog.com/2014/08/https-as-ranking-signal.html"&gt;Google has announced it will likely prefer sites using HTTPS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The site is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;My &lt;a class="reference external" href="https://agilitycourses.com"&gt;agilitycourses.com&lt;/a&gt; website is served by &lt;a class="reference external" href="https://nginx.org/en/"&gt;nginx&lt;/a&gt; proxying to &lt;a class="reference external" href="http://gunicorn.org/"&gt;gunicorn&lt;/a&gt; running my &lt;a class="reference external" href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; application. I'll be adding user accounts soon so I wanted to convert the site to be more secure by using HTTPS encryption. Also &lt;a class="reference external" href="https://webmasters.googleblog.com/2014/08/https-as-ranking-signal.html"&gt;Google has announced it will likely prefer sites using HTTPS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The site is running on Ubuntu 14.04 LTS. I won't recount the whole process, I followed some great resources and I'll discuss a couple adjustments that might be helpful to others.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;I basically followed the instructions in this excellent Digital Ocean tutorial: &lt;a class="reference external" href="https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-14-04"&gt;How To Secure Nginx with Let's Encrypt on Ubuntu 14.04&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I confirmed via the &lt;a class="reference external" href="https://www.ssllabs.com/ssltest/analyze.html"&gt;SSL Labs SSL Server Test&lt;/a&gt; that my IPv4 and IPv6 server configurations had &amp;quot;A+&amp;quot; ratings.&lt;/li&gt;
&lt;li&gt;While looking for other SSL testing sites I came across &lt;a class="reference external" href="https://securityheaders.io/"&gt;securityheaders.io&lt;/a&gt; developed by &lt;a class="reference external" href="https://scotthelme.co.uk/"&gt;Scott Helme&lt;/a&gt;. My initial score was a sad &amp;quot;D&amp;quot;. The site has snippets for NGINX and Apache configuration changes and in depth articles describing the how and the why.&lt;/li&gt;
&lt;li&gt;While investigating the changes to the HTTP Headers to improve my test score I came across this &lt;a class="reference external" href="https://github.com/jonnybarnes/nginx-conf"&gt;nginx-conf GitHub repository&lt;/a&gt;. Specifically the idea of putting the header settings into an &lt;a class="reference external" href="https://github.com/jonnybarnes/nginx-conf/blob/master/conf/includes/security-headers.conf"&gt;NGINX include file&lt;/a&gt;. I have several other domains on the same server and will also be converting them. I used that idea to include ssl and header configurations into any virtual host.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here's my &lt;cite&gt;/etc/nginx/ssl.conf&lt;/cite&gt; file:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# From https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-ubuntu-14-04
ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
ssl_prefer_server_ciphers on;
ssl_dhparam /etc/ssl/certs/dhparam.pem;
ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA';
ssl_session_timeout 1d;
ssl_session_cache shared:SSL:50m;
ssl_stapling on;
ssl_stapling_verify on;
&lt;/pre&gt;
&lt;p&gt;And my &lt;cite&gt;/etc/nginx/security_headers.conf&lt;/cite&gt; file:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# See https://github.com/jonnybarnes/nginx-conf/blob/master/conf/includes/security-headers.conf
add_header X-Xss-Protection &amp;quot;1; mode=block&amp;quot;;
add_header X-Content-Type-Options &amp;quot;nosniff&amp;quot;;
add_header Content-Security-Policy &amp;quot;default-src https: data: 'unsafe-inline' 'unsafe-eval'&amp;quot;;
add_header Strict-Transport-Security max-age=15768000;
&lt;/pre&gt;
&lt;p&gt;So my server blocks with all these edits are now:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# redirect http://www.tld and http://tld to https://www.tld
server {
    listen 80;
    listen [::]:80;
    server_name www.agilitycourses.com agilitycourses.com;

    # letsencrypt location
    location ^~ /.well-known/ {
        allow all;
        root /usr/share/nginx/html/;
    }
    location / {
        return 301 https://www.agilitycourses.com$request_uri;
    }
}

# redirect https://tld to https://www.tld
server {
    listen 443 ssl;
    listen [::]:443 ipv6only=on ssl;

    server_name agilitycourses.com;
    # certificates are needed here too
    ssl_certificate /etc/letsencrypt/live/agilitycourses.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/agilitycourses.com/privkey.pem;
    return 301 https://www.agilitycourses.com$request_uri;
}

server {
    listen 443 ssl;
    listen [::]:443 ssl;

    server_name www.agilitycourses.com;
    root /home/agilitycourses/production/current/;

    ssl_certificate /etc/letsencrypt/live/agilitycourses.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/agilitycourses.com/privkey.pem;

    include /etc/nginx/ssl.conf;
    include /etc/nginx/security_headers.conf;
    ...
}
&lt;/pre&gt;
&lt;p&gt;So now I have an &amp;quot;A&amp;quot; score from &lt;cite&gt;securityheaders.io&lt;/cite&gt;&lt;/p&gt;
&lt;ol class="arabic" start="5"&gt;
&lt;li&gt;&lt;p class="first"&gt;The Digital Ocean tutorial sets up a root crontab entry to automatically update the SSL Certificate. I decided to also update the letsencrypt client software automatically:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# m h  dom mon dow   command
20 2 * * 1 cd /opt/letsencrypt &amp;amp;&amp;amp; git pull
30 2 * * 1 /opt/letsencrypt/letsencrypt-auto renew &amp;gt;&amp;gt; /var/log/le-renew.log
35 2 * * 1 /etc/init.d/nginx reload
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;The last change I made was to pass along the presence/absence of HTTPS from NGINX to Gunicorn/Django via the &lt;cite&gt;X-Forwarded-Proto&lt;/cite&gt; header as &lt;a class="reference external" href="https://docs.djangoproject.com/en/1.10/topics/security/#ssl-https"&gt;described in the Django SSL/HTTPS docs&lt;/a&gt;&lt;/p&gt;
&lt;pre class="literal-block"&gt;
location &amp;#64;proxy-to-app {
    proxy_pass http://agilitycourses-production-gunicorn;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header Host $host;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header Accept-Encoding &amp;quot;&amp;quot;;
    proxy_read_timeout 120;
    proxy_send_timeout 120;
    ...
}
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Based on the Django recommendations I also made these changes in my &lt;cite&gt;settings.py&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
# SSL settings
SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')
SECURE_BROWSER_XSS_FILTER = True
SESSION_COOKIE_SECURE = True
CSRF_COOKIE_SECURE = True
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Even with a lot of web browsing to learn about these settings the whole process only took a couple hours.
Now that I've done it once (and updated my &lt;a class="reference external" href="http://www.fabfile.org/"&gt;Fabric fabfile.py&lt;/a&gt;) it will be easier to convert my other domains.&lt;/p&gt;
</content><category term="nginx"></category><category term="ubuntu"></category><category term="https"></category><category term="ssl"></category><category term="django"></category><category term="gunicorn"></category><category term="letsencrypt"></category></entry><entry><title>NGINX CGI Parameter Gotcha</title><link href="https://tech.agilitynerd.com/nginx-cgi-parameter-gotcha.html" rel="alternate"></link><published>2016-07-31T12:02:00-05:00</published><updated>2016-07-31T12:02:00-05:00</updated><author><name>Steve Schwarz</name></author><id>tag:tech.agilitynerd.com,2016-07-31:/nginx-cgi-parameter-gotcha.html</id><summary type="html">&lt;p&gt;When I first started the &lt;a class="reference external" href="http://agilitynerd.com"&gt;agilitynerd&lt;/a&gt;  blog in 2004 I had my &lt;a class="reference external" href="http://blosxom.sourceforge.net/"&gt;Blosxom&lt;/a&gt; blogging CGI script running via &lt;a class="reference external" href="http://httpd.apache.org/"&gt;Apache&lt;/a&gt;. Later on I moved all my sites to &lt;a class="reference external" href="https://nginx.org/en/"&gt;nginx&lt;/a&gt; or took advantage of nginx's caching features to have it act as a proxy in front of Apache. I finally decided to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When I first started the &lt;a class="reference external" href="http://agilitynerd.com"&gt;agilitynerd&lt;/a&gt;  blog in 2004 I had my &lt;a class="reference external" href="http://blosxom.sourceforge.net/"&gt;Blosxom&lt;/a&gt; blogging CGI script running via &lt;a class="reference external" href="http://httpd.apache.org/"&gt;Apache&lt;/a&gt;. Later on I moved all my sites to &lt;a class="reference external" href="https://nginx.org/en/"&gt;nginx&lt;/a&gt; or took advantage of nginx's caching features to have it act as a proxy in front of Apache. I finally decided to remove Apache entirely and that meant solving running CGI scripts using nginx.&lt;/p&gt;
&lt;p&gt;After some googling I found FastCGI and &lt;a class="reference external" href="https://www.nginx.com/resources/wiki/start/topics/examples/fcgiwrap/"&gt;fcgiwrap&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I'm on Ubuntu so installation was as easy as:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo apt-get install fcgiwrap
&lt;/pre&gt;
&lt;p&gt;That setup the init script that starts the fcgi daemon. To run the cgi script(s) nginx has to be configured to parse apart the incoming URL, execute the appropriate script and pass along any arguments needed by the CGI script. Sounds easy.&lt;/p&gt;
&lt;p&gt;I only want to support running a single CGI script: &lt;tt class="docutils literal"&gt;index.cgi&lt;/tt&gt; and pass along the path after the root of URL as the argument to the script. Most examples are more generic and parse out any cgi script and any arguments.&lt;/p&gt;
&lt;p&gt;The key built-in to nginx to do the splitting is &lt;tt class="docutils literal"&gt;fastcgi_split_path_info&lt;/tt&gt; which takes a regex with two captured groups to parse out the script name and the arguments. These are stored in the &lt;tt class="docutils literal"&gt;$fastcgi_script_name&lt;/tt&gt; and the &lt;tt class="docutils literal"&gt;$fastcgi_path_info&lt;/tt&gt; variables respectively. This &lt;a class="reference external" href="https://www.digitalocean.com/community/tutorials/understanding-and-implementing-fastcgi-proxying-in-nginx"&gt;Digital Ocean article&lt;/a&gt; discusses FastCGI and has an excellent discussion of the variables available and also used by &lt;tt class="docutils literal"&gt;fcgiwrap&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;So I created this configuration file that matches &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;http://agilitynerd.com/blog/foo.html&lt;/span&gt;&lt;/tt&gt; and invokes &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;/home/agilitynerd/cgi-bin/index.cgi&lt;/span&gt; foo.html&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
location /blog/ {
    root /home/agilitynerd/cgi-bin/;

    fastcgi_split_path_info ^(/blog)(.*)$;
    include /etc/nginx/fastcgi_params;
    fastcgi_param DOCUMENT_ROOT /home/agilitynerd/cgi-bin/;
    fastcgi_param SCRIPT_NAME index.cgi$fastcgi_path_info;

    # Fastcgi socket
    fastcgi_pass  unix:/var/run/fcgiwrap.socket;
}
&lt;/pre&gt;
&lt;p&gt;You'll notice: &lt;tt class="docutils literal"&gt;include /etc/nginx/fastcgi_params&lt;/tt&gt; is used to get default values for the &lt;tt class="docutils literal"&gt;fastcgi_param&lt;/tt&gt; variables.&lt;/p&gt;
&lt;p&gt;And it didn't work. I kept getting errors:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Cannot get script name, are DOCUMENT_ROOT and SCRIPT_NAME (or SCRIPT_FILENAME) set and is the script executable?&amp;quot;
&lt;/pre&gt;
&lt;p&gt;Clearly I'm setting &lt;tt class="docutils literal"&gt;DOCUMENT_ROOT&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;SCRIPT_NAME&lt;/tt&gt;. After almost a day of googling and testing (during which I found this helpful article on &lt;a class="reference external" href="https://blog.martinfjordvald.com/2013/06/debugging-nginx-errors/"&gt;nginx debugging&lt;/a&gt;) I temporarily commented out &lt;tt class="docutils literal"&gt;fastcgi_pass&lt;/tt&gt;, and returned the variables.&lt;/p&gt;
&lt;p&gt;I found that they were being set as I expected... Strange!&lt;/p&gt;
&lt;p&gt;Then I came across this &lt;a class="reference external" href="https://www.digitalocean.com/community/tutorials/understanding-and-implementing-fastcgi-proxying-in-nginx"&gt;Digital Ocean article&lt;/a&gt; where they have a critical discussion on overriding variables in which they state:&lt;/p&gt;
&lt;blockquote&gt;
This inconsistency and unpredictability means that you cannot and should not rely on the backend to correctly interpret your intentions when setting the same parameter more than one time. The only safe solution is to only declare each parameter once. This also means that there is no such thing as safely overriding a default value with the fastcgi_param directive.&lt;/blockquote&gt;
&lt;p&gt;So in my case I commented out &lt;tt class="docutils literal"&gt;DOCUMENT_ROOT&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;SCRIPT_FILENAME&lt;/tt&gt; in the &lt;tt class="docutils literal"&gt;/etc/nginx/fastcgi_params&lt;/tt&gt; file, reloaded nginx, and voila! Everything worked. Hope this helps you if you run in to the same problem.&lt;/p&gt;
</content><category term="nginx"></category><category term="cgi"></category><category term="fcgiwrap"></category><category term="ubuntu"></category><category term="apache"></category></entry><entry><title>Upgrading Pelican and Migrating to GitHub Pages</title><link href="https://tech.agilitynerd.com/upgrading-pelican-migrating-gh-pages.html" rel="alternate"></link><published>2015-11-29T15:00:00-06:00</published><updated>2015-11-29T15:00:00-06:00</updated><author><name>Steve Schwarz</name></author><id>tag:tech.agilitynerd.com,2015-11-29:/upgrading-pelican-migrating-gh-pages.html</id><summary type="html">&lt;p&gt;I've been using &lt;a class="reference external" href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt; for this blog for almost three years with source and output stored in a GitHub repository. The output files were then checked out and hosted as static content behind an &lt;a class="reference external" href="http://www.nginx.com/"&gt;NGINX web server&lt;/a&gt; on my VPS. Since I set that up GitHub introduced &lt;a class="reference external" href="https://pages.github.com/"&gt;GitHub Pages&lt;/a&gt; with …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been using &lt;a class="reference external" href="http://blog.getpelican.com/"&gt;Pelican&lt;/a&gt; for this blog for almost three years with source and output stored in a GitHub repository. The output files were then checked out and hosted as static content behind an &lt;a class="reference external" href="http://www.nginx.com/"&gt;NGINX web server&lt;/a&gt; on my VPS. Since I set that up GitHub introduced &lt;a class="reference external" href="https://pages.github.com/"&gt;GitHub Pages&lt;/a&gt; with support for custom domains and all the &amp;quot;cool kids&amp;quot; started hosting their static web sites right on GitHub.&lt;/p&gt;
&lt;p&gt;I had some free time this weekend and decided to see what it would take to upgrade my Pelican version to the latest (3.6.3) and host my files on GitHub Pages. I had four steps to perform:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Create a new environment with the latest Pelican&lt;/li&gt;
&lt;li&gt;Update my content files for the changes in Pelican versions&lt;/li&gt;
&lt;li&gt;Put output files into GitHub and verify them on GitHub Pages&lt;/li&gt;
&lt;li&gt;Move my subdomain to point to my GitHub Pages&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="section" id="create-new-environment"&gt;
&lt;h2&gt;Create New Environment&lt;/h2&gt;
&lt;p&gt;I didn't want to screw up my existing/working virtual environment so I created a new one containing Pelican and &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;ghp-import&lt;/span&gt;&lt;/tt&gt; which does all the work of updating the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gh-pages&lt;/span&gt;&lt;/tt&gt; branch with the output:&lt;/p&gt;
&lt;pre class="code bash literal-block"&gt;
&lt;span class="c1"&gt;# Create a new virtualenv
&lt;/span&gt;mkvirtualenv pelican-new
&lt;span class="c1"&gt;# Install pelican and ghp-import:
&lt;/span&gt;pip install pelican ghp-import
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="update-content"&gt;
&lt;h2&gt;Update Content&lt;/h2&gt;
&lt;p&gt;This was arguably the most painful part as I wasn't using appropriate reStructuredText markup for my images and the location of images required removing &lt;tt class="docutils literal"&gt;/static&lt;/tt&gt; from the path. So my markup went from:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;
.. raw:: html

   &amp;lt;div class=&amp;quot;thumbnail&amp;quot;&amp;gt;

&amp;lt;img src=&amp;quot;/static/images/myimage.png&amp;quot; /&amp;gt;

.. raw:: html

   &amp;lt;/div&amp;gt;
&lt;/pre&gt;
&lt;p&gt;to this (which includes adding a missing &lt;tt class="docutils literal"&gt;alt&lt;/tt&gt; tag):&lt;/p&gt;
&lt;pre class="code literal-block"&gt;
.. class:: thumbnail
.. figure:: {filename}/images/myimage.png
   :alt: Clever alt image text goes here
&lt;/pre&gt;
&lt;p&gt;Those changes were mostly mechanical and using &lt;tt class="docutils literal"&gt;figure::&lt;/tt&gt; in place of &lt;tt class="docutils literal"&gt;raw::&lt;/tt&gt; also cleaned up the mark up. I tested the changes locally and confirmed all modified pages where displaying correctly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="convert-to-github-pages"&gt;
&lt;h2&gt;Convert to GitHub Pages&lt;/h2&gt;
&lt;p&gt;This setup is now documented in the Pelican docs on &lt;a class="reference external" href="http://docs.getpelican.com/en/3.6.3/tips.html#publishing-to-github"&gt;Publish to GitHub&lt;/a&gt; and is easy.&lt;/p&gt;
&lt;p&gt;It looks like the clever setup is to put the source for the blog in the &lt;tt class="docutils literal"&gt;master&lt;/tt&gt; branch and then check the output of running &lt;tt class="docutils literal"&gt;pelican&lt;/tt&gt; into a branch called &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gh-pages&lt;/span&gt;&lt;/tt&gt;.  The &lt;a class="reference external" href="https://github.com/davisp/ghp-import"&gt;ghp-import python package&lt;/a&gt; does all the work of creating and updating the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gh-pages&lt;/span&gt;&lt;/tt&gt; branch from the &lt;tt class="docutils literal"&gt;output&lt;/tt&gt; directory for you!&lt;/p&gt;
&lt;p&gt;The first thing I did was to switch to my &lt;tt class="docutils literal"&gt;master&lt;/tt&gt; branch and then remove the &lt;tt class="docutils literal"&gt;content&lt;/tt&gt; directory and all of it's files:&lt;/p&gt;
&lt;pre class="code bash literal-block"&gt;
git checkout master
rm -rf output
&lt;/pre&gt;
&lt;p&gt;Then I edited the &lt;tt class="docutils literal"&gt;.gitignore&lt;/tt&gt; file to exclude the &lt;tt class="docutils literal"&gt;output&lt;/tt&gt; directory.&lt;/p&gt;
&lt;p&gt;I wanted to keep my existing blog working until I worked out all the kinks in the migration. So I delayed pointing DNS to the GitHub pages. That meant I needed to temporarily change the URL of the blog to match where it will be hosted on GitHub pages. So I edited the &lt;tt class="docutils literal"&gt;publishconf.py&lt;/tt&gt; configuration file and changed the &lt;tt class="docutils literal"&gt;SITEURL&lt;/tt&gt; temporarily from:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="n"&gt;SITEURL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'http://steve.agilitynerd.com'&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;to it's location on GitHub Pages:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="n"&gt;SITEURL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'http://saschwarz.github.io/steve-agilitynerd'&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Get the URL by clicking on the Settings tab for the GitHub repository:&lt;/p&gt;
&lt;div class="thumbnail figure"&gt;
&lt;img alt="Screenshot of GitHub settings showing URL for GitHub pages" src="https://tech.agilitynerd.com/images/github-pages-url.png" /&gt;
&lt;/div&gt;
&lt;p&gt;Now that the &lt;tt class="docutils literal"&gt;master&lt;/tt&gt; branch is set up I checked in and commited the changes:&lt;/p&gt;
&lt;pre class="code bash literal-block"&gt;
git commit -a -m&lt;span class="s2"&gt;&amp;quot;Migration to GitHub Pages&amp;quot;&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Now I followed the instructions in the Pelican docs to generate the output and add it to the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gh-pages&lt;/span&gt;&lt;/tt&gt; branch via &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;ghp-import&lt;/span&gt;&lt;/tt&gt; (except they show using &lt;tt class="docutils literal"&gt;pelicanconf.py&lt;/tt&gt; which I use for local development)&lt;/p&gt;
&lt;pre class="code bash literal-block"&gt;
pelican content -o output -s publishconf.py
ghp-import output
&lt;/pre&gt;
&lt;p&gt;or, since I opted to have Pelican automation setup, I did:&lt;/p&gt;
&lt;pre class="code bash literal-block"&gt;
make github
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;ghp-import&lt;/span&gt;&lt;/tt&gt; committed and pushed the output to GitHub and I tested that files/images were correctly being served by going to the GitHub Pages URL in my browser.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="move-subdomain-to-github-pages"&gt;
&lt;h2&gt;Move Subdomain to GitHub Pages&lt;/h2&gt;
&lt;p&gt;This step is well documented in the GitHub help page: &lt;a class="reference external" href="https://help.github.com/articles/about-custom-domains-for-github-pages-sites/"&gt;About custom domains for GitHub Pages sites&lt;/a&gt;. In my case I was already using a subdomain for my Pelican blogs so I just followed their instructions.&lt;/p&gt;
&lt;p&gt;On my VPS's DNS configuration screen I deleted my subdomain's &lt;tt class="docutils literal"&gt;A&lt;/tt&gt; record pointing to my VPS and added a &lt;tt class="docutils literal"&gt;CNAME&lt;/tt&gt; record pointing to my GitHub &lt;cite&gt;.io&lt;/cite&gt; account.&lt;/p&gt;
&lt;p&gt;Then &lt;strong&gt;don't followed these instructions:&lt;/strong&gt; &lt;a class="reference external" href="https://help.github.com/articles/adding-a-cname-file-to-your-repository/"&gt;Adding a CNAME file to your repository&lt;/a&gt; to setup a &lt;tt class="docutils literal"&gt;CNAME&lt;/tt&gt; file in the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gh-pages&lt;/span&gt;&lt;/tt&gt; branch. The instructions work but &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;ghp-import&lt;/span&gt;&lt;/tt&gt; deletes the content of the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gh-pages&lt;/span&gt;&lt;/tt&gt; branch before re-adding files and that deletes the &lt;tt class="docutils literal"&gt;CNAME&lt;/tt&gt; file you just added!&lt;/p&gt;
&lt;p&gt;After some googling I found Tip #2 in the &lt;a class="reference external" href="http://docs.getpelican.com/en/latest/tips.html#extra-tips"&gt;Pelican Tips&lt;/a&gt; and followed their instructions. I added the following to my &lt;tt class="docutils literal"&gt;publishconf.py&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="n"&gt;STATIC_PATHS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'images'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'extra/CNAME'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;EXTRA_PATH_METADATA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'extra/CNAME'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;'path'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'CNAME'&lt;/span&gt;&lt;span class="p"&gt;},}&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Then I created the &lt;tt class="docutils literal"&gt;CNAME&lt;/tt&gt; file in the new &lt;tt class="docutils literal"&gt;content/extra&lt;/tt&gt; directory with the name of my subdomain in it:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;steve.agilitynerd.com&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Undo the edit to &lt;tt class="docutils literal"&gt;publishconf.py&lt;/tt&gt; so it uses the subdomain name:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="n"&gt;SITEURL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'http://steve.agilitynerd.com'&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Commit that edit to the &lt;tt class="docutils literal"&gt;master&lt;/tt&gt; branch and then regenerate the output and commit it to &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;gh-pages&lt;/span&gt;&lt;/tt&gt; branches:&lt;/p&gt;
&lt;pre class="code bash literal-block"&gt;
git commit -a -m&lt;span class="s2"&gt;&amp;quot;Done with migration to sub domain&amp;quot;&lt;/span&gt;
git push
make github
&lt;/pre&gt;
&lt;p&gt;Opened the browser to my subdomain and verified that images and links within the site were working correctly. I went back to my VPS and disabled the subdomains from NGINX and deleted the blog check outs to free some resources.  Two fewer websites to maintain on my VPS!&lt;/p&gt;
&lt;/div&gt;
</content><category term="pelican"></category><category term="github"></category><category term="python"></category></entry><entry><title>Notes on Configuring Postfix on Ubuntu Gutsy to Send Email via Google Apps</title><link href="https://tech.agilitynerd.com/notes-on-configuring-postfix-on-ubuntu-gutsy-1.html" rel="alternate"></link><published>2008-01-05T23:53:00-06:00</published><updated>2008-01-05T23:53:00-06:00</updated><author><name>Steve Schwarz</name></author><id>tag:tech.agilitynerd.com,2008-01-05:/notes-on-configuring-postfix-on-ubuntu-gutsy-1.html</id><summary type="html">&lt;p&gt;Here are some notes I took on configuring my &lt;a class="reference external" href="http://www.slicehost.com/"&gt;Slicehost&lt;/a&gt; Ubuntu Gutsy
installation to use Postfix to send emails via Google Apps. I am far
from an expert on postfix configuration but maybe these notes will be
helpful to others needing this configuration.&lt;/p&gt;
&lt;p&gt;These sites contain the key information:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://behindmyscreen.newsvine.com/_news/2006/12/31/501615-configuringubuntu-postfix-and-gmail-in-101-easy-steps"&gt;Behind …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Here are some notes I took on configuring my &lt;a class="reference external" href="http://www.slicehost.com/"&gt;Slicehost&lt;/a&gt; Ubuntu Gutsy
installation to use Postfix to send emails via Google Apps. I am far
from an expert on postfix configuration but maybe these notes will be
helpful to others needing this configuration.&lt;/p&gt;
&lt;p&gt;These sites contain the key information:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://behindmyscreen.newsvine.com/_news/2006/12/31/501615-configuringubuntu-postfix-and-gmail-in-101-easy-steps"&gt;Behind My Screen - Configuring[Ubuntu] Postfix and Gmail in 10+1
Easy Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://prantran.blogspot.com/2007/01/getting-postfix-to-work-on-ubuntu-with.html"&gt;The Prancing Tarantula - Getting Postfix to work on Ubuntu with
Gmail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://souptonuts.sourceforge.net/postfix_tutorial.html"&gt;Mike Chirico - Gmail on Home Linux Box using Postfix and Fetchmail&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I basically followed the Behind My Screen tutorial (read the comments
too) with the updates from The Prancing Tarantula and the following
changes.&lt;/p&gt;
&lt;p&gt;My Ubuntu server install didn't have the Thawte certificates installed
by default so I installed them:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo aptitude install ca-certificates
&lt;/pre&gt;
&lt;p&gt;Then you can append that file to your /etc/postfix/cacert.pem If you
&amp;quot;sudo su&amp;quot; before doing the append to the file you won't get messed up by
the shell:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ sudo su
# cat /etc/ssl/certs/Thawte_Premium_Server_CA.pem &amp;gt;&amp;gt;
/etc/postfix/cacert.pem
&lt;/pre&gt;
&lt;p&gt;Since I have Google Apps setup for my domain I don't just want to relay
email as &amp;quot;&lt;a class="reference external" href="mailto:user&amp;#64;gmail.com"&gt;user&amp;#64;gmail.com&lt;/a&gt;&amp;quot;, I want the email to be sent as though it
came from my domain (&amp;quot;&lt;a class="reference external" href="mailto:me&amp;#64;agilitynerd.com"&gt;me&amp;#64;agilitynerd.com&lt;/a&gt;&amp;quot;). This requires some simple
changes to the config files.&lt;/p&gt;
&lt;p&gt;In my transport file I have:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
agilitynerd.com smtp:[smtp.gmail.com]:587
&lt;/pre&gt;
&lt;p&gt;In my generic file I have:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
demo&amp;#64;myservername.agilitynerd.com me&amp;#64;agilitynerd.com
&lt;/pre&gt;
&lt;p&gt;Where &amp;quot;demo&amp;quot; is the login name and &amp;quot;myservername&amp;quot; is my slicename. In my
sasl_passwd file I have:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[smtp.gmail.com]:587 me&amp;#64;agilitynerd.com:me_gmail_account_password
&lt;/pre&gt;
&lt;p&gt;After restarting postix you can test sending email from your server:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ sudo aptitude install mailx
$ mailx -s &amp;quot;test email&amp;quot; someotheraccount&amp;#64;gmail.com &amp;lt;
~/sometestfile_to_send
&lt;/pre&gt;
&lt;p&gt;Check your logfiles for errors/warnings:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo tail /var/log/mail.\*
&lt;/pre&gt;
&lt;p&gt;I hope these notes might help folks &amp;quot;get over the hump&amp;quot; if they are
setting up the same configuration.&lt;/p&gt;
</content><category term="postfix"></category><category term="slicehost"></category><category term="ubuntu"></category></entry><entry><title>Backup Your Data Lately?</title><link href="https://tech.agilitynerd.com/backup-your-data-lately-1.html" rel="alternate"></link><published>2004-08-20T19:00:00-05:00</published><updated>2004-08-20T19:00:00-05:00</updated><author><name>Steve Schwarz</name></author><id>tag:tech.agilitynerd.com,2004-08-20:/backup-your-data-lately-1.html</id><summary type="html">&lt;p&gt;A few weeks ago I bought a &lt;a class="reference external" href="http://www.ximeta.com/products/network_drives/netdisk/index.php"&gt;Ximeta NetDisk&lt;/a&gt; 120 GB external hard drive
to back up the data on my home computers. There are a number of vendors
making hard drives that support USB connections; this model is unique in
that it also supports direct ethernet connections. Unlike more …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few weeks ago I bought a &lt;a class="reference external" href="http://www.ximeta.com/products/network_drives/netdisk/index.php"&gt;Ximeta NetDisk&lt;/a&gt; 120 GB external hard drive
to back up the data on my home computers. There are a number of vendors
making hard drives that support USB connections; this model is unique in
that it also supports direct ethernet connections. Unlike more expensive
Network Attached Storage devices (more than $ 1000) this unit only cost
a little more than the hard drive itself; about $ 150. The only downside
is that it uses it's own proprietary network protocol which requires
installation of a driver on any computers using the drive.&lt;/p&gt;
&lt;p&gt;My goal was to put this drive on my network switch and back up data from
my XP, WinME, RedHat 8.0, and Win95 machines. It turns out this was a
little trickier than I expected. So I spent the better part of the
morning on a cool, grey Chicago day making this all work.&lt;/p&gt;
&lt;p&gt;The disk comes formatted for NTFS but in order for it to be shared on
the older Windows platforms and my Linux machine it needs to be
reformatted to use a FAT32 file system. Windows XP doesn't support
formatting drives for FAT32 so I had to install the Ximeta driver
software on my WinME machine and install the NetDisk on my network.&lt;/p&gt;
&lt;p&gt;Running fdisk and reformatting the hard drive is the only &amp;quot;scary&amp;quot; aspect
of the installation. Choosing the wrong drive or partition would be a
&lt;em&gt;bad&lt;/em&gt; thing. This &lt;a class="reference external" href="http://www.ximeta.com/support/guides/netdisk/ndas/98seme/05.php"&gt;document&lt;/a&gt; gives a good step by step description.&lt;/p&gt;
&lt;p&gt;I was skeptical that running fdisk over the network would work
correctly, but it did. At this point I was able to view the drive on
both my WinME and XP systems and copy data to the drive as if it was
locally connected.&lt;/p&gt;
&lt;p&gt;I was pretty sure making the drive work for Linux would be difficult.
Unfortunately, the PDF documents from the Ximeta website are unreadable
as they require installing the Korean Acrobat extensions... thankfully
Google has its &amp;quot;View has HTML&amp;quot; facility which let me read the RedHat
instructions. The docs on the install CDROM are viewable (but don't
include the RedHat docs).&lt;/p&gt;
&lt;p&gt;I'll spare you all the trial and error but after downloading the &lt;a class="reference external" href="http://www.ximeta.com/support/downloads/red_hat_8/index.php"&gt;driver
RPM&lt;/a&gt; from the website and installing it I couldn't configure and
connect to the drive on the network. It could be that ports required for
their protocol aren't opened on my Linux machine, but Ximeta doesn't
give any information on what ports are used by their driver. The admin
tool gives some cryptic error messages that Googling and the docs didn't
explain (the docs recommend reinstalling the drivers for any errors...).
I ended up connecting the drive directly via USB and was able to mount
the drive and backup my user and system accounts to the disk.&lt;/p&gt;
&lt;p&gt;So it looks like my goal of leaving the drive on the network and copying
to it from any computer will only work for Windows machines. But at
least I have a mechanism for backing up all my machines, that is easy
enough that I'll use it all the time. My next step is looking into
configuring &lt;a class="reference external" href="http://rsync.samba.org"&gt;rsync&lt;/a&gt; or a similar mechanism to only backup the changed
files to the NetDisk.&lt;/p&gt;
&lt;p&gt;In summary, I'd recommend this drive for anyone who is using Windows XP;
it is plug and play for that operating system. If you are computer savvy
you can make this hard drive play with other systems too.&lt;/p&gt;
&lt;p&gt;However you do it, &lt;strong&gt;backup your computer!&lt;/strong&gt;&lt;/p&gt;
</content><category term="sysadmin"></category></entry></feed>